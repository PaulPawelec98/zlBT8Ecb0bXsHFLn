

















# add parent to directory
import os
import json
import sys

# get parent of directory
parent_dir = os.path.abspath("..")

# add if false
if parent_dir not in sys.path:
    sys.path.append(parent_dir)


%%capture

# packages
import os
import sys
import ast
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
from collections import Counter

# For Jupyter.
# import itables
# from itables import show
# itables.init_notebook_mode()

# project modules
from apzivaproject3 import (  # base modules for scripts
    dataset,
    features,
    train,
    predict
)

import apzivaproject3.config  # shared configuration file

from apzivaproject3.config import (  # default project paths
    PROCESSED_DATA_DIR,
    MODELS_DIR,
    SETTINGS_FILE,
    INTERIM_DATA_DIR
    )

pd.set_option('display.max_colwidth', None)  # Show full cell content
pd.set_option('display.max_columns', None)   # Show all columns
pd.set_option('display.width', 0)            # Let pandas auto-wrap


# Lets me Skip Cells
from IPython.core.magic import register_cell_magic
@register_cell_magic
def skip(line, cell):
    return





# Settings

# run project bool
make_project = False

# Inputs - Find Relavant Candidates to these targets...
target = 'aspiring human resources'
penalty = 'data analyst'
stars = [10, 14, 15, 18, 200, 500, 700, 900 ,1000]  # for ranknet to have these candidates win.

# Read User Settings
with open(SETTINGS_FILE, 'r') as f:
    settings = json.load(f)

# Update User Settings
settings['target_settings']['target_string'] = target
settings['target_settings']['penalty_string'] = penalty
settings['starred_candidates'] = stars

# Write Updated Settings
with open(SETTINGS_FILE, 'w') as f:
    json.dump(settings, f, indent=4)


%%capture

# run main modules
if make_project == True:
#    dataset.main()
#    features.main()
    train.main()
    predict.main()






# final dataset with scores/fitness
df = pd.read_csv(PROCESSED_DATA_DIR / 'df_with_scores.csv')


# cleaned corpus text
with open(PROCESSED_DATA_DIR / "corpus.txt", 'r') as file:
    corpus = file.read()
    
corpus_list = corpus.split(" ")


Counter(corpus_list).most_common()[:10]








df_sim_scores_benchmark = pd.read_csv(PROCESSED_DATA_DIR / 'df_with_scores.csv')


# show(df_sim_scores_benchmark.drop(columns=['country', 'job_title_list']))


df_sim_scores_benchmark.head(10)








df_penalty = pd.read_csv(PROCESSED_DATA_DIR /  "df_with_scores_data_analyst_scientist_penalty.csv")


df_penalty = df_penalty.sort_values('sbert', ascending=False)
df_penalty.loc[:,['job_title_string', 'sbert', 'sbertwpenalty1']].head(10)


df_penalty = df_penalty.sort_values('sbertwpenalty1', ascending=True)
df_penalty.loc[:,['job_title_string', 'sbert', 'sbertwpenalty1']].head(10)











import torch
embeddings = torch.load(str(MODELS_DIR / "mybert/mysbertembeddings.pt"), weights_only=False)


# PCA Reduction
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Standardizing the features
scaler = StandardScaler()
data_scaled = scaler.fit_transform(embeddings)

# Apply PCA to reduce to 3 components
pca = PCA(n_components=3)
principal_components = pca.fit_transform(data_scaled)


# Total Variance Kept
explained_variance = pca.explained_variance_ratio_
total_variance = explained_variance.sum()
total_variance





# Alternatives to PCA (TSNE and UMAP)
from sklearn.manifold import TSNE
import umap.umap_ as umap
from sklearn.preprocessing import StandardScaler

# t-SNE
tsne = TSNE(n_components=3, perplexity=30, n_iter=1000, random_state=1)
tsne_components = tsne.fit_transform(data_scaled)

# UMAP
umap_model = umap.UMAP(n_components=3, n_neighbors=15, min_dist=0.25, metric='euclidean', random_state=1)
umap_components = umap_model.fit_transform(data_scaled)


# Variables
texts = list(df.job_title_string)
texts.append(target)


# sort for nearest connections
df_sorted = df.copy()
df_sorted = df_sorted.sort_values(by="sbert", ascending=False)
df_sorted.index
components = umap_components.copy()
components_sorted = umap_components.copy()
components_sorted = components_sorted[df_sorted.index]


# Kmeans for Looking at Different Groups
from sklearn.cluster import KMeans

distortions = []
for k in range(1, 10):
    kmeans = KMeans(n_clusters=k, random_state=1)
    kmeans.fit(components_sorted)
    distortions.append(kmeans.inertia_)

plt.plot(range(1, 10), distortions)
plt.xlabel('Number of clusters')
plt.ylabel('Distortion')
plt.title('Scores')
plt.show()


kmeans = KMeans(n_clusters=8, random_state=1)
kmeans.fit(components)


color_key = {
    0: 'blue',
    1: 'purple',
    2: 'orange',
    3: 'green',
    4: 'yellow',
    5: 'pink',
    6: 'teal',
    7: 'grey'
}

point_colors = [color_key[label] for label in kmeans.labels_[:-1]]


# Import dependencies
import plotly
import plotly.graph_objs as go

# Configure Plotly to be rendered inline in the notebook.
plotly.offline.init_notebook_mode()

# Configure the trace.
scatter_trace = go.Scatter3d(
    x=components[:-1,0],  # <-- Put your data instead
    y=components[:-1,1],  # <-- Put your data instead
    z=components[:-1,2],  # <-- Put your data instead
    mode='markers',
    text=texts,
    marker={'size': 5, 'opacity': 0.8, 'color':point_colors},
    hoverinfo='text'  # Only show text when hovered
)

# Add a label at the specific target point
target_trace = go.Scatter3d(
    x=[components[-1, 0]],  # X coordinate of the target point
    y=[components[-1, 1]],  # Y coordinate of the target point
    z=[components[-1, 2]],  # Z coordinate of the target point
    mode='markers+text',  # Only display text
    text=texts[-1].upper(),  # The label text
    textposition='top center',  # Position of the label
    textfont=dict(
        family='Arial',  # You can customize the font here
        size=12,         # Adjust font size
        color='black',   # Adjust font color
        weight='bold'    # Make the text bold
    ),
    marker={'size': 15, 'color': 'red'},  # Red marker for the target point
)

# Function to create line traces to the best scores
def create_lines(k):
    lines = []
    for i in range(1, k + 1):  # Exclude the target point itself
        
        line_trace = go.Scatter3d(
            x=[components[-1, 0], components_sorted[i, 0]],
            y=[components[-1, 1], components_sorted[i, 1]],
            z=[components[-1, 2], components_sorted[i, 2]],
            mode='lines',
            hoverinfo="none",
            line=dict(color='red', width=5),
            showlegend=False
        )
        lines.append(line_trace)
        
    return lines

# Initialize with only 1 connection
initial_lines = create_lines(10)

# Origin Reference
origin_line = go.Scatter3d(
    x=[components[-1, 0], 0],
    y=[components[-1, 1], 0],
    z=[components[-1, 2], 0],
    mode='lines',
    hoverinfo="none",
    line=dict(color='grey', width=5),
    showlegend=False
)

origin_point = go.Scatter3d(
    x=[0,0],  # <-- Put your data instead
    y=[0,0],  # <-- Put your data instead
    z=[0,0],  # <-- Put your data instead
    mode='markers',
    text='Origin',
    marker={'size': 5, 'opacity': 0.8, 'color': 'grey'},  # Color target point red, others blue
    hoverinfo='text'  # Only show text when hovered
)

data = [scatter_trace, target_trace, origin_line, origin_point] + initial_lines

# Configure the layout.
layout = go.Layout(
    title="3D Scatter Plot: SBERT Embeddings",
    scene={
        'xaxis': {'title': 'PC 1'},  # Label for the x-axis
        'yaxis': {'title': 'PC 2'},  # Label for the y-axis
        'zaxis': {'title': 'PC 3'}   # Label for the z-axis
    },
    margin={'l': 0, 'r': 0, 'b': 0, 't': 40},  # Adjust margin for the title
    hovermode='closest',  # Enable hover on the closest point
)

fig = go.Figure(data=data, layout=layout)

plotly.offline.iplot(fig)








df_groups = pd.DataFrame({'job_title_string': texts, 'group_id': kmeans.labels_})
df_groups['job_title_list'] = [string.split(' ') for string in df_groups['job_title_string']]


def most_common_word(job_lists):
    all_words = []
    for item in job_lists:
        if isinstance(item, list):
            all_words.extend(item)
    return Counter(all_words).most_common(10) if all_words else None

# Apply per group
most_common_by_group = pd.DataFrame(df_groups.groupby('group_id')['job_title_list'].apply(most_common_word))
most_common_by_group = most_common_by_group.rename(columns={'job_title_list': 'Most Common Words with Count'})
most_common_by_group["num_obs"] = df_groups.groupby('group_id')['job_title_list'].apply(lambda x: len(x))
most_common_by_group['colors'] = color_key.values()
most_common_by_group








df_rank_data = pd.read_csv(PROCESSED_DATA_DIR / 'rankdata.csv')
df_rank_data['job_title_string'] = df_sim_scores_benchmark['job_title_string']
cols = ['job_title_string'] + [col for col in df_rank_data.columns if col != 'job_title_string']  # reorder columns.
df_rank_data = df_rank_data[cols]


# itables.init_notebook_mode()
# show(df_rank_data)


df_rank_data.head(10)


%%skip
class RankNet(nn.Module):
    def __init__(self, input_size, hidden_size):
        super().__init__()
        print(f"""Initializing RankNet with input_size={input_size},
              hidden_size={hidden_size}"""
              )
        self.input = nn.Linear(input_size, hidden_size)
        self.hidden = nn.Linear(hidden_size, hidden_size)
        self.output = nn.Linear(hidden_size, 1)
        self.activation = nn.ReLU()  # cuts negative scores.

    def forward(self, x1, x2):
        h1 = self.activation(self.hidden(self.activation(self.input(x1))))
        h2 = self.activation(self.hidden(self.activation(self.input(x2))))
        return self.output(h1) - self.output(h2)





%%skip
class RankDatasetStars(Dataset):
    # for ranknet with pairs
    def __init__(self, X, y, star):
        self.X = torch.tensor(X.values, dtype=torch.float32)
        self.y = torch.tensor(y.values, dtype=torch.float32)
        self.star = torch.tensor(star.values, dtype=torch.float32)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx], self.star[idx]





%%skip
# Training loop
for epoch in range(epoch_range):

    train_iter = iter(train_loader)  # shuffles the data.
    model.train().to(device)

    for rand in range(0, random_range, 1):

        try:
            x1, y1, star1 = next(train_iter)
            x2, y2, star2 = next(train_iter)
        except StopIteration:
            print("Train iterator exhausted. Breaking early.")
            break

        x1, x2 = x1.to(device), x2.to(device)
        y1, y2 = y1.to(device), y2.to(device)
        star1, star2 = star1.to(device), star2.to(device)

        optimizer.zero_grad()

        diff = model(x1, x2)

        # let's ignore comparing against two starred candidates.
        if star1 == 1 and star2 == 1:
            continue

        target = torch.tensor(
            [[1.0]]
            if (y1 - (y1 * star2)) > (y2 - (y2 * star1))
            else [[0.0]]
            ).to(device)

        loss = criterion(diff, target)
        loss.backward()
        optimizer.step()

        print(f"Epoch {epoch}, Loss: {loss.item()}")








df.loc[stars,['job_title_string', 'country']]





df_ranknet = pd.read_csv(PROCESSED_DATA_DIR / 'df_with_rank.csv')
df_ranknet = df_ranknet.sort_values('score', ascending=False)
df_ranknet.head(15)














%%skip

# flan-tokenizer --------------------------------------------------------------
tokenizer = T5Tokenizer.from_pretrained(
    "google/flan-t5-small"
    )

model = T5ForConditionalGeneration.from_pretrained(
    "google/flan-t5-small",
    device_map="auto",
    torch_dtype=torch.float16  # Reduce memory
)

job_tokens = []

for job in jobs:
    input_ids = tokenizer(
        job, return_tensors="pt", padding="longest", truncation=True
        ).input_ids.to("cuda")
    job_tokens.append(input_ids)

torch.save(job_tokens, INTERIM_DATA_DIR / 'flan_job_tensors.pt')
# loaded_list = torch.load('tensor_list.pt')
# -----------------------------------------------------------------------------


%%skip

class TokenPairs(Dataset):
    # to predict with all possible pairs already made.
    def __init__(self, tokens, static_tokens, mask):
        """


        Parameters
        ----------
        tokens : list of tensors
            Tokens that change for each prompt.
        static_tokens : list of tensors
            Tokens that repeat for each prompt.
        mask : list of tensors
            This is just each index where we should concat our static prompts
            to our jobs.

        Returns
        -------
        None.

        """

        self.tokens = [t.to("cpu") for t in tokens]
        self.static_tokens = [t.to("cpu") for t in static_tokens]
        self.num_samples = len(self.tokens)
        self.pairs = list(itertools.combinations(range(self.num_samples), 2))
        self.mask = mask

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        i, j = self.pairs[idx]
        inputs = self.static_tokens.copy()

        inputs[self.mask[0]],  inputs[self.mask[1]] = (
            torch.cat(
                [self.static_tokens[self.mask[0]], self.tokens[i]], dim=1
                ),
            torch.cat(
                [self.static_tokens[self.mask[1]], self.tokens[j]], dim=1
                )
            )

        inputs = torch.cat(inputs, dim=1)
        return inputs.squeeze(0), int(i), int(j)





%%skip

def predict_flan():

    # ... skiped lines

    scores = {idx: 0 for idx in df.index.tolist()}

    for batch_idx, (prompts, i, j) in enumerate(tokenloaders):
        batch_start = time.time()

        # ----------------- Inference ------------------
        outputs = model.generate(prompts.to("cuda"), max_new_tokens=1)

        # ----------------- Post-processing ------------------
        for k in range(len(prompts)):
            out = tokenizer.decode(outputs[k])
            print(out)
            try:
                digit = int(re.search(r'\d+', out).group(0))
            except AttributeError:
                print(f"No digit found in output: {out!r}")
                continue

            if digit == 1:
                scores[int(i[k])] += 1
            else:
                scores[int(j[k])] += 1

        batch_end = time.time()
        elapsed = batch_end - batch_start
        batch_times.append(elapsed)

        avg_time = sum(batch_times) / len(batch_times)
        batches_left = total_batches - batch_idx
        eta = avg_time * batches_left

        print(
            f"\n Batch {batch_idx}/{total_batches} | "
            f"Batch Time: {elapsed:.2f}s | "
            f"ETA: {eta / 60:.2f} minutes ({eta:.1f} seconds)\n"
        )

    print(f" All done in {time.time() - overall_start:.2f} seconds")

    return scores








df_flan_scores = pd.read_csv(INTERIM_DATA_DIR / 'flan_scores.csv')
df_llms = df.loc[:,['job_title_string']].copy()
df_llms['flan_scores'] = df_flan_scores['score']
df_llms = df_llms.sort_values('flan_scores', ascending=False)
df_llms.head(15)








# Example Prompt

target_string = 'aspiring human resources'

job_titles = df['job_title_string'][:50].tolist()

list_string = "\n".join(
    f"{i+1}. {title}" for i, title in enumerate(job_titles)
    )

prompt = f"""
You are given a list of job titles and a set of target words.

**Task**: Rank the job titles from most to least relevant to the target words.
Do not include duplicates.

Return the output as a list in the form of index positions (1-based),\
 the number of items in this must be equal to {len(job_titles)}. Format should\
 look like this:
[4, 3, 2, 6, 1, 5]

**Now do the same for the following input:**

Target words: {target_string}

Job titles:
{list_string}

Return only the final ranked list of index positions:
"""








%%skip

# Load Model
model_name = "Qwen/Qwen2.5-3B-Instruct"

tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=CACHE_DIR)

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,  # Set False for 8-bit
    bnb_4bit_compute_dtype=torch.float16,
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    cache_dir=CACHE_DIR,
    quantization_config=quant_config,
    device_map="auto",
    )


%%skip

# Prompt Model
messages = [
    {"role": "system", "content": "You are an expert human resources assistant that excels in ranking candidates."},
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([text], return_tensors="pt").to(device)

generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=2048,
    temperature=0.2,
    # repetition_penalty=1.1,
    # do_sample=True,
)

generated_ids = [
    output_ids[len(input_ids):]
    for input_ids, output_ids
    in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]


# This is an example response I recieved from QWEN 2.5
response = '[5, 3, 2, 6, 1, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]'
lst = ast.literal_eval(response)
new_order = df['job_title_string'][lst].copy()
new_order.head(15)


lst[:15]








%%skip

# %% WindowSlider Class


class WindowSlider():
    def __init__(self, df, window_size=10, slide_size=5):
        self.window_size = window_size
        self.slide_size = slide_size
        self.df = df.copy()
        self.starting_indexes = df.index.tolist()
        self.current_window = self.starting_indexes[:self.window_size]
        self.end = False

    def slide(self, slide_size=None, direction='down'):
        if direction == 'down':
            # Move the window by `slide_size` with overlap
            if slide_size is None:
                slide_size = self.slide_size

            new_start = self.current_window[0] + slide_size

            if new_start + self.window_size > len(self.df):
                print('slide, Reached End of DataFrame')
                self.end = True
                return None

            # Update the window
            self.current_window = (
                self.df.index[new_start:new_start + self.window_size].tolist()
                )

            return self.current_window

        elif direction == 'up':
            # Move the window upwards by `slide_size` with overlap
            if slide_size is None:
                slide_size = self.slide_size

            # Calculate the new start index for upwards sliding
            new_start = self.current_window[0] - slide_size

            # Check if the new start is before the beginning of the DataFrame
            if new_start < 0:
                print('slide, Reached Beginning of DataFrame')
                return None

            # Update the window by slicing from the new start index
            self.current_window = (
                self.df.index[new_start:new_start + self.window_size].tolist()
            )

            return self.current_window

        else:
            raise ValueError("direction must be 'down' or 'up'")

    def swap(self, new_idx):
        # Get the old indices (from the current window)
        old_idx = self.current_window

        # Iterate over pairs of old and new indices
        for old, new in zip(old_idx, new_idx):
            # Swap the rows
            temp = self.df.loc[old].copy()
            self.df.loc[old] = self.df.loc[new]
            self.df.loc[new] = temp

        return [old_idx, new_idx]  # Return the swapped indexes

    def reset_starting(self):
        self.current_window = self.starting_indexes[:self.window_size].tolist()

    def get_current_window(self):
        # Returns the current window indexes
        return self.current_window

    def get_df(self):
        return self.df





df_sliding_scores = pd.read_csv(PROCESSED_DATA_DIR / 'df_upside_2.csv')


# itables.init_notebook_mode()
# show(df_sliding_scores)


df_sliding_scores.head(20)











df_simscores_qwen34b = pd.read_csv(PROCESSED_DATA_DIR / 'df_with_llm_sim_scores_2025-05-10_19-07-11.csv')


# itables.init_notebook_mode()
# show(df_simscores_qwen34b)


df_simscores_qwen34b.head(10)











df_simscores_llms = pd.read_csv(PROCESSED_DATA_DIR /  'df_with_llm_sim_scores_2025-05-12_22-25-50.csv')


df_simscores_llms = df_simscores_llms.sort_values(by='generated_sim_score', ascending=False)
df_simscores_llms.head(15)


df_simscores_llms.index[:15].sort_values()


df_simscores_llms = df_simscores_llms.sort_values(by='average_sim_score', ascending=False)
df_simscores_llms.head(15)


df_simscores_llms.index[:15].sort_values()











%%skip

# Creating the Vector Database

# Create chroma and setup collection ------------------------------------------
chroma_client = chromadb.PersistentClient(
    path=str(COLLECTION_DATA_DIR)
)

# collection = chroma_client.create_collection(
#     name="job_titles_collection",
#     embedding_function=embedding_function
#     )

# collection.add(
#     documents=job_tiles,
#     ids=job_ids,
#     )
# -----------------------------------------------------------------------------

# Return Collection -----------------------------------------------------------
collection = chroma_client.get_collection(
    name="job_titles_collection",
    embedding_function=embedding_function
    )
# -----------------------------------------------------------------------------

# Return Most Relavant Terms --------------------------------------------------

target_words = "aspiring human resources"

results = collection.query(
    query_texts=[target_words],
    n_results=10
)
print(results)
# -----------------------------------------------------------------------------















